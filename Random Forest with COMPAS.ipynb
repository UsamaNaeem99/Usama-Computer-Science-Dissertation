{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63084476-1a63-42bd-9001-d4c3cf3fb61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kagglehub pandas scikit-learn fairlearn aif360 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09aaee54-b0cc-43cc-b44b-adcca3b9a79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (60843, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arhamsoft\\AppData\\Local\\Temp\\ipykernel_35680\\1568927048.py:27: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['DateOfBirth'] = pd.to_datetime(df['DateOfBirth'], errors='coerce')\n",
      "C:\\Users\\Arhamsoft\\AppData\\Local\\Temp\\ipykernel_35680\\1568927048.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Screening_Date'] = pd.to_datetime(df['Screening_Date'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Performance on COMPAS:\n",
      "Accuracy : 0.7929\n",
      "Precision: 0.6849\n",
      "Recall   : 0.7477\n",
      "F1 Score : 0.7150\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from datetime import datetime\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\Arhamsoft\\Desktop\\Talha Talib Thesis\\compas-scores-raw.csv\")\n",
    "print(\"Original shape:\", df.shape)\n",
    "\n",
    "# âœ… Filtering\n",
    "df = df[\n",
    "    (df['IsCompleted'] == 1) &\n",
    "    (df['ScoreText'].isin(['Low', 'Medium', 'High']))\n",
    "]\n",
    "\n",
    "# ðŸŽ¯ Target variable\n",
    "df['two_year_recid'] = df['ScoreText'].map({'Low': 0, 'Medium': 1, 'High': 1})\n",
    "\n",
    "# âœ… Filter to African-American and Caucasian only\n",
    "df = df[df['Ethnic_Code_Text'].isin(['African-American', 'Caucasian'])]\n",
    "df['race_binary'] = df['Ethnic_Code_Text'].map({'Caucasian': 0, 'African-American': 1})\n",
    "df['sex_binary'] = df['Sex_Code_Text'].map({'Male': 1, 'Female': 0})\n",
    "\n",
    "# ðŸŽ‚ Derive age\n",
    "df['DateOfBirth'] = pd.to_datetime(df['DateOfBirth'], errors='coerce')\n",
    "df['Screening_Date'] = pd.to_datetime(df['Screening_Date'], errors='coerce')\n",
    "df['age'] = (df['Screening_Date'] - df['DateOfBirth']).dt.days // 365\n",
    "\n",
    "# Drop rows with missing values in key fields\n",
    "df = df.dropna(subset=['age', 'LegalStatus', 'CustodyStatus', 'RecSupervisionLevel'])\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "df = pd.get_dummies(df, columns=['LegalStatus', 'CustodyStatus', 'RecSupervisionLevel'], drop_first=True)\n",
    "\n",
    "# Select features\n",
    "features = ['sex_binary', 'age'] + [col for col in df.columns if col.startswith('LegalStatus_') or col.startswith('CustodyStatus_') or col.startswith('RecSupervisionLevel_')]\n",
    "X = df[features]\n",
    "y = df['two_year_recid']\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "acc = accuracy_score(y_test, y_pred_rf)\n",
    "prec = precision_score(y_test, y_pred_rf)\n",
    "rec = recall_score(y_test, y_pred_rf)\n",
    "f1 = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Random Forest Performance on COMPAS:\")\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88f03909-e725-4a4e-b1b6-92f609b59999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reweighed Random Forest Performance:\n",
      "Accuracy : 0.7904\n",
      "Precision: 0.6881\n",
      "Recall   : 0.7257\n",
      "F1 Score : 0.7064\n",
      "\n",
      "Fairness Metrics (Reweighing):\n",
      "Statistical Parity Difference : 0.2502\n",
      "Equal Opportunity Difference : 0.1760\n",
      "Disparate Impact Ratio       : 2.0942\n"
     ]
    }
   ],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference, selection_rate, MetricFrame\n",
    "\n",
    "# Define protected attribute\n",
    "protected = df['race_binary']\n",
    "\n",
    "# Align protected attribute using label-based indexing\n",
    "protected_train = protected.loc[y_train.index].reset_index(drop=True)\n",
    "protected_test = protected.loc[y_test.index].reset_index(drop=True)\n",
    "\n",
    "# Reset index\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "# Combine into DataFrames\n",
    "Xy_train = X_train.copy()\n",
    "Xy_train['target'] = y_train\n",
    "Xy_train['race'] = protected_train\n",
    "\n",
    "Xy_test = X_test.copy()\n",
    "Xy_test['target'] = y_test\n",
    "Xy_test['race'] = protected_test\n",
    "\n",
    "# Convert to BinaryLabelDataset\n",
    "train_bld = BinaryLabelDataset(df=Xy_train, label_names=['target'], protected_attribute_names=['race'], favorable_label=0, unfavorable_label=1)\n",
    "test_bld = BinaryLabelDataset(df=Xy_test, label_names=['target'], protected_attribute_names=['race'], favorable_label=0, unfavorable_label=1)\n",
    "\n",
    "# Apply Reweighing\n",
    "rw = Reweighing(unprivileged_groups=[{'race': 0}], privileged_groups=[{'race': 1}])\n",
    "rw.fit(train_bld)\n",
    "train_rw = rw.transform(train_bld)\n",
    "\n",
    "# Train Random Forest with reweighing\n",
    "rf_rw = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_rw.fit(X_train, y_train, sample_weight=train_rw.instance_weights)\n",
    "\n",
    "# Predict\n",
    "y_pred_rw = rf_rw.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "acc = accuracy_score(y_test, y_pred_rw)\n",
    "prec = precision_score(y_test, y_pred_rw)\n",
    "rec = recall_score(y_test, y_pred_rw)\n",
    "f1 = f1_score(y_test, y_pred_rw)\n",
    "\n",
    "print(\"Reweighed Random Forest Performance:\")\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")\n",
    "\n",
    "# Fairness Metrics\n",
    "fair_metrics = MetricFrame(\n",
    "    metrics={\n",
    "        \"accuracy\": accuracy_score,\n",
    "        \"precision\": precision_score,\n",
    "        \"recall\": recall_score,\n",
    "        \"f1\": f1_score,\n",
    "        \"selection_rate\": selection_rate\n",
    "    },\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_rw,\n",
    "    sensitive_features=protected_test\n",
    ")\n",
    "\n",
    "spd = demographic_parity_difference(y_test, y_pred_rw, sensitive_features=protected_test)\n",
    "eod = equalized_odds_difference(y_test, y_pred_rw, sensitive_features=protected_test)\n",
    "di_ratio = fair_metrics.by_group['selection_rate'].max() / fair_metrics.by_group['selection_rate'].min()\n",
    "\n",
    "print(\"\\nFairness Metrics (Reweighing):\")\n",
    "print(f\"Statistical Parity Difference : {spd:.4f}\")\n",
    "print(f\"Equal Opportunity Difference : {eod:.4f}\")\n",
    "print(f\"Disparate Impact Ratio       : {di_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6d972fc-4850-4036-bfa3-40250fe56289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arhamsoft\\anaconda3\\envs\\tf-env\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equalized Odds Postprocessing Random Forest Performance:\n",
      "Accuracy : 1.0000\n",
      "Precision: 1.0000\n",
      "Recall   : 1.0000\n",
      "F1 Score : 1.0000\n",
      "\n",
      "Fairness Metrics (Equalized Odds Postprocessing):\n",
      "Statistical Parity Difference : 0.1784\n",
      "Equal Opportunity Difference : 0.0000\n",
      "Disparate Impact Ratio       : 1.7159\n"
     ]
    }
   ],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from fairlearn.metrics import MetricFrame, demographic_parity_difference, equalized_odds_difference, selection_rate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Split X, y, and protected attribute together to preserve alignment\n",
    "protected = df['race_binary']\n",
    "X_train, X_test, y_train, y_test, prot_train, prot_test = train_test_split(\n",
    "    X_scaled, y, protected, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Reset indices\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "prot_train = prot_train.reset_index(drop=True)\n",
    "prot_test = prot_test.reset_index(drop=True)\n",
    "\n",
    "# Wrap test data for aif360\n",
    "test_df = X_test.copy()\n",
    "test_df['two_year_recid'] = y_test\n",
    "test_df['race'] = prot_test\n",
    "\n",
    "# Predict probabilities using calibrated classifier\n",
    "calibrated_rf = CalibratedClassifierCV(estimator=rf_model, method='sigmoid', cv='prefit')\n",
    "calibrated_rf.fit(X_train, y_train)\n",
    "y_probs = calibrated_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Create BinaryLabelDataset with scores\n",
    "test_bld = BinaryLabelDataset(\n",
    "    df=test_df,\n",
    "    label_names=['two_year_recid'],\n",
    "    protected_attribute_names=['race'],\n",
    "    favorable_label=0,\n",
    "    unfavorable_label=1\n",
    ")\n",
    "test_bld_pred = test_bld.copy()\n",
    "test_bld_pred.scores = y_probs.reshape(-1, 1)\n",
    "\n",
    "# Apply Equalized Odds Postprocessing\n",
    "eopp = EqOddsPostprocessing(unprivileged_groups=[{'race': 0}], privileged_groups=[{'race': 1}])\n",
    "eopp = eopp.fit(test_bld, test_bld_pred)\n",
    "test_eq = eopp.predict(test_bld_pred)\n",
    "y_pred_eq = test_eq.labels.ravel()\n",
    "\n",
    "# Evaluate performance\n",
    "acc = accuracy_score(y_test, y_pred_eq)\n",
    "prec = precision_score(y_test, y_pred_eq)\n",
    "rec = recall_score(y_test, y_pred_eq)\n",
    "f1 = f1_score(y_test, y_pred_eq)\n",
    "\n",
    "print(\"Equalized Odds Postprocessing Random Forest Performance:\")\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")\n",
    "\n",
    "# Fairness metrics\n",
    "fair_metrics = MetricFrame(\n",
    "    metrics={\"accuracy\": accuracy_score, \"precision\": precision_score, \"recall\": recall_score,\n",
    "             \"f1\": f1_score, \"selection_rate\": selection_rate},\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_eq,\n",
    "    sensitive_features=prot_test\n",
    ")\n",
    "spd = demographic_parity_difference(y_test, y_pred_eq, sensitive_features=prot_test)\n",
    "eod = equalized_odds_difference(y_test, y_pred_eq, sensitive_features=prot_test)\n",
    "di_ratio = fair_metrics.by_group['selection_rate'].max() / fair_metrics.by_group['selection_rate'].min()\n",
    "\n",
    "print(\"\\nFairness Metrics (Equalized Odds Postprocessing):\")\n",
    "print(f\"Statistical Parity Difference : {spd:.4f}\")\n",
    "print(f\"Equal Opportunity Difference : {eod:.4f}\")\n",
    "print(f\"Disparate Impact Ratio       : {di_ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea26728b-e69d-41a3-b7c2-45386cf12065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-env]",
   "language": "python",
   "name": "conda-env-tf-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
